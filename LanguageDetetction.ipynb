{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJgcJ7NO6ul7"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix\n",
        "import numpy as np\n",
        "import speech_recognition as sr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9sOda0562Lw",
        "outputId": "f0f0f515-566f-470c-d92b-5e84047fbfda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-097a7adde9bd>:1: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  data = pd.read_csv('/content/sample_data/sentences.csv', sep='\\t',encoding='utf8', index_col=0, names=['lang','text'], error_bad_lines = False)\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv('/content/sample_data/sentences.csv', sep='\\t',encoding='utf8', index_col=0, names=['lang','text'], error_bad_lines = False)\n",
        "lang = ['deu', 'eng', 'fra', 'ita', 'por', 'spa']\n",
        "data = data[data['lang'].isin(lang)]\n",
        "\n",
        "data_trim = pd.DataFrame(columns=['lang', 'text'])\n",
        "\n",
        "for l in lang:\n",
        "    language_trim = data[data['lang'] == l]\n",
        "    if len(language_trim) > 50000:\n",
        "        language_trim = language_trim.sample(50000, random_state=100)\n",
        "    data_trim = pd.concat([data_trim, language_trim])\n",
        "\n",
        "#create random train, valid, test split\n",
        "data_shuffle = data_trim.sample(frac=1, random_state=100)\n",
        "\n",
        "train = data_shuffle.iloc[:210000]\n",
        "valid = data_shuffle.iloc[210000:270000]\n",
        "test = data_shuffle.iloc[270000:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOIZeKMh7gOm"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxSQK2Qc630y"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def get_trigrams(corpus,n_feat=200):\n",
        "    #fit the n-gram model\n",
        "    vectorizer = CountVectorizer(analyzer='char',\n",
        "                            ngram_range=(3, 3)\n",
        "                            ,max_features=n_feat)\n",
        "    \n",
        "    X = vectorizer.fit_transform(corpus)\n",
        "    \n",
        "    #Get model feature names\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    \n",
        "    return feature_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMevuWGm65_0"
      },
      "outputs": [],
      "source": [
        "features = {}\n",
        "features_set = set()\n",
        "\n",
        "for l in lang:\n",
        "    \n",
        "    #get corpus filtered by language\n",
        "    corpus = train[train.lang==l]['text']\n",
        "    \n",
        "    #get 200 most frequent trigrams\n",
        "    trigrams = get_trigrams(corpus)\n",
        "    \n",
        "    #add to dict and set\n",
        "    features[l] = trigrams \n",
        "    features_set.update(trigrams)\n",
        "\n",
        "    \n",
        "#create vocabulary list using feature set\n",
        "vocab = dict()\n",
        "for i,f in enumerate(features_set):\n",
        "    vocab[f]=i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tg0obl3Q678S"
      },
      "outputs": [],
      "source": [
        "#train count vectoriser using vocabulary\n",
        "vectorizer = CountVectorizer(analyzer='char',\n",
        "                             ngram_range=(3, 3),\n",
        "                            vocabulary=vocab)\n",
        "\n",
        "#create feature matrix for training set\n",
        "corpus = train['text']   \n",
        "X = vectorizer.fit_transform(corpus)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "train_feat = pd.DataFrame(data=X.toarray(),columns=feature_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpp4ms2N6-Bo"
      },
      "outputs": [],
      "source": [
        "#Scale feature matrix \n",
        "train_min = train_feat.min()\n",
        "train_max = train_feat.max()\n",
        "train_feat = (train_feat - train_min)/(train_max-train_min)\n",
        "\n",
        "#Add target variable \n",
        "train_feat['lang'] = list(train['lang'])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create feature matrix for validation set\n",
        "corpus = valid['text']   \n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "valid_feat = pd.DataFrame(data=X.toarray(),columns=feature_names)\n",
        "valid_feat = (valid_feat - train_min)/(train_max-train_min)\n",
        "valid_feat['lang'] = list(valid['lang'])\n",
        "\n",
        "#create feature matrix for test set\n",
        "corpus = test['text']   \n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "test_feat = pd.DataFrame(data=X.toarray(),columns=feature_names)\n",
        "test_feat = (test_feat - train_min)/(train_max-train_min)\n",
        "test_feat['lang'] = list(test['lang'])"
      ],
      "metadata": {
        "id": "e9KlhOACv0Gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "langs = ['deu', 'eng', 'fra', 'ita', 'por', 'spa']\n",
        "\n",
        "# Create a LabelEncoder object\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "# Fit the encoder to the list of languages\n",
        "encoder.fit_transform(langs)\n",
        "\n",
        "def encode(y):\n",
        "    \n",
        "    y_encoded = encoder.transform(y)\n",
        "    y_dummy = np_utils.to_categorical(y_encoded)\n",
        "    \n",
        "    return y_dummy"
      ],
      "metadata": {
        "id": "X9-yb648v8bV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_feat.drop('lang',axis=1)\n",
        "y = encode(train_feat['lang'])\n",
        "\n",
        "#Define model\n",
        "model = Sequential()\n",
        "model.add(Dense(500, input_dim=670, activation='relu'))\n",
        "model.add(Dense(500, activation='relu'))\n",
        "model.add(Dense(250, activation='relu'))\n",
        "model.add(Dense(6, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "#Train model\n",
        "model.fit(x, y, epochs=4, batch_size=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKXkzwgUwQvm",
        "outputId": "b30014cd-f8b7-4f0c-ad66-7dd86b264a4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "6563/6563 [==============================] - 36s 4ms/step - loss: 0.0989 - accuracy: 0.9637\n",
            "Epoch 2/4\n",
            "6563/6563 [==============================] - 28s 4ms/step - loss: 0.0562 - accuracy: 0.9789\n",
            "Epoch 3/4\n",
            "6563/6563 [==============================] - 28s 4ms/step - loss: 0.0362 - accuracy: 0.9863\n",
            "Epoch 4/4\n",
            "6563/6563 [==============================] - 28s 4ms/step - loss: 0.0260 - accuracy: 0.9904\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2e7b179cf0>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a SpeechRecognition recognizer\n",
        "!pip install PyAudio‑0.2.11‑cp37‑cp37m‑win_amd64.whl\n",
        "\n",
        "r = sr.Recognizer()\n",
        "\n",
        "# Record audio from the microphone\n",
        "with sr.Microphone() as source:\n",
        "    print(\"Speak now...\")\n",
        "    audio = r.listen(source)\n",
        "\n",
        "# Use Google's speech recognition to get the spoken text\n",
        "text = r.recognize_google(audio, language=langs)\n",
        "\n",
        "# Preprocess the spoken input\n",
        "text = [text]\n",
        "\n",
        "# Vectorize the spoken input using the same vectorizer used for training\n",
        "input_matrix = vectorizer.transform(text)\n",
        "\n",
        "# Scale the input matrix using the minimum and maximum values from the training data\n",
        "input_matrix = (input_matrix - train_min) / (train_max - train_min)\n",
        "\n",
        "# Predict the language of the spoken input\n",
        "predicted_lang = encoder.inverse_transform(np.argmax(model.predict(input_matrix), axis=-1))[0]\n",
        "\n",
        "# Print the predicted language\n",
        "print(\"Spoken language: \", predicted_lang)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        },
        "id": "WOOsJXTqFctl",
        "outputId": "bd9694a1-83fd-4ef5-c718-eded8481cc74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple/, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyaudio\n",
            "  Using cached PyAudio-0.2.13.tar.gz (46 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pyaudio\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for pyaudio \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for pyaudio (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for pyaudio\u001b[0m\u001b[31m\n",
            "\u001b[0mFailed to build pyaudio\n",
            "\u001b[31mERROR: Could not build wheels for pyaudio, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/speech_recognition/__init__.py\u001b[0m in \u001b[0;36mget_pyaudio\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0;32mimport\u001b[0m \u001b[0mpyaudio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyaudio'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-fe96061f9821>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Record audio from the microphone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMicrophone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Speak now...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlisten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/speech_recognition/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, device_index, sample_rate, chunk_size)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;31m# set up PyAudio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyaudio_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_pyaudio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyaudio_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyAudio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/speech_recognition/__init__.py\u001b[0m in \u001b[0;36mget_pyaudio\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mpyaudio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Could not find PyAudio; check installation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mdistutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLooseVersion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mLooseVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mLooseVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"0.2.11\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: Could not find PyAudio; check installation"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize CountVectorizer with the vocabulary\n",
        "vectorizer = CountVectorizer(analyzer='char', ngram_range=(3, 3), vocabulary=vocab)\n",
        "\n",
        "# Fit and transform the training corpus using the vectorizer\n",
        "input_text = input(\"Enter a sentence: \")\n",
        "train_feat_matrix = vectorizer.transform([input_text])\n",
        "input_feat = pd.DataFrame(data=train_feat_matrix.toarray(),columns=feature_names)\n",
        "input_feat = (input_feat - train_min)/(train_max-train_min)\n",
        "\n",
        "input_pred = model.predict(input_feat)\n",
        "input_lang = encoder.inverse_transform(np.argmax(input_pred, axis=-1))\n",
        "\n",
        "print(\"The language of the input sentence is:\", input_lang[0])"
      ],
      "metadata": {
        "id": "oTAzLWLei-CP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = test_feat.drop('lang', axis=1)\n",
        "y_test = test_feat['lang']\n",
        "\n",
        "# Load the model from the saved file\n",
        "#model = tf.keras.models.load_model('my_model.h5')\n",
        "\n",
        "#Get predictions on test set\n",
        "labels = np.argmax(model.predict(x_test), axis=-1)\n",
        "predictions = encoder.inverse_transform(labels)\n",
        "\n",
        "#Accuracy on test set\n",
        "accuracy = accuracy_score(y_test,predictions)\n",
        "print(accuracy)\n",
        "\n",
        "#Create confusion matrix\n",
        "lang = ['eng','fra','spa', 'hin', 'ara', 'jpn']\n",
        "conf_matrix = confusion_matrix(y_test,predictions)\n",
        "conf_matrix_df = pd.DataFrame(conf_matrix,columns=lang,index=lang)\n",
        "\n",
        "#Plot confusion matrix heatmap\n",
        "plt.figure(figsize=(10, 10), facecolor='w', edgecolor='k')\n",
        "sns.set(font_scale=1.5)\n",
        "sns.heatmap(conf_matrix_df,cmap='coolwarm',annot=True,fmt='.5g',cbar=False)\n",
        "plt.xlabel('Predicted',fontsize=22)\n",
        "plt.ylabel('Actual',fontsize=22)\n"
      ],
      "metadata": {
        "id": "-QsRXRt0v-RA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}