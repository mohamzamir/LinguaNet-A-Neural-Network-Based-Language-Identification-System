{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJgcJ7NO6ul7"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score,confusion_matrix\n",
        "import numpy as np\n",
        "import speech_recognition as sr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9sOda0562Lw"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/content/sample_data/sentences.csv', sep='\\t',encoding='utf8', index_col=0, names=['lang','text'], error_bad_lines = False)\n",
        "lang = ['deu', 'eng', 'fra', 'ita', 'por', 'spa']\n",
        "data = data[data['lang'].isin(lang)]\n",
        "\n",
        "data_trim = pd.DataFrame(columns=['lang', 'text'])\n",
        "\n",
        "for l in lang:\n",
        "    language_trim = data[data['lang'] == l]\n",
        "    if len(language_trim) > 50000:\n",
        "        language_trim = language_trim.sample(50000, random_state=100)\n",
        "    data_trim = pd.concat([data_trim, language_trim])\n",
        "\n",
        "#create random train, valid, test split\n",
        "data_shuffle = data_trim.sample(frac=1, random_state=100)\n",
        "\n",
        "train = data_shuffle.iloc[:210000]\n",
        "valid = data_shuffle.iloc[210000:270000]\n",
        "test = data_shuffle.iloc[270000:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOIZeKMh7gOm"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxSQK2Qc630y"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def get_trigrams(corpus,n_feat=200):\n",
        "    #fit the n-gram model\n",
        "    vectorizer = CountVectorizer(analyzer='char',\n",
        "                            ngram_range=(3, 3)\n",
        "                            ,max_features=n_feat)\n",
        "    \n",
        "    X = vectorizer.fit_transform(corpus)\n",
        "    \n",
        "    #Get model feature names\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    \n",
        "    return feature_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMevuWGm65_0"
      },
      "outputs": [],
      "source": [
        "features = {}\n",
        "features_set = set()\n",
        "\n",
        "for l in lang:\n",
        "    \n",
        "    #get corpus filtered by language\n",
        "    corpus = train[train.lang==l]['text']\n",
        "    \n",
        "    #get 200 most frequent trigrams\n",
        "    trigrams = get_trigrams(corpus)\n",
        "    \n",
        "    #add to dict and set\n",
        "    features[l] = trigrams \n",
        "    features_set.update(trigrams)\n",
        "\n",
        "    \n",
        "#create vocabulary list using feature set\n",
        "vocab = dict()\n",
        "for i,f in enumerate(features_set):\n",
        "    vocab[f]=i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tg0obl3Q678S"
      },
      "outputs": [],
      "source": [
        "#train count vectoriser using vocabulary\n",
        "vectorizer = CountVectorizer(analyzer='char',\n",
        "                             ngram_range=(3, 3),\n",
        "                            vocabulary=vocab)\n",
        "\n",
        "#create feature matrix for training set\n",
        "corpus = train['text']   \n",
        "X = vectorizer.fit_transform(corpus)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "train_feat = pd.DataFrame(data=X.toarray(),columns=feature_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpp4ms2N6-Bo"
      },
      "outputs": [],
      "source": [
        "#Scale feature matrix \n",
        "train_min = train_feat.min()\n",
        "train_max = train_feat.max()\n",
        "train_feat = (train_feat - train_min)/(train_max-train_min)\n",
        "\n",
        "#Add target variable \n",
        "train_feat['lang'] = list(train['lang'])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create feature matrix for validation set\n",
        "corpus = valid['text']   \n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "valid_feat = pd.DataFrame(data=X.toarray(),columns=feature_names)\n",
        "valid_feat = (valid_feat - train_min)/(train_max-train_min)\n",
        "valid_feat['lang'] = list(valid['lang'])\n",
        "\n",
        "#create feature matrix for test set\n",
        "corpus = test['text']   \n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "test_feat = pd.DataFrame(data=X.toarray(),columns=feature_names)\n",
        "test_feat = (test_feat - train_min)/(train_max-train_min)\n",
        "test_feat['lang'] = list(test['lang'])"
      ],
      "metadata": {
        "id": "e9KlhOACv0Gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "langs = ['deu', 'eng', 'fra', 'ita', 'por', 'spa']\n",
        "\n",
        "# Create a LabelEncoder object\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "# Fit the encoder to the list of languages\n",
        "encoder.fit_transform(langs)\n",
        "\n",
        "def encode(y):\n",
        "    \n",
        "    y_encoded = encoder.transform(y)\n",
        "    y_dummy = np_utils.to_categorical(y_encoded)\n",
        "    \n",
        "    return y_dummy"
      ],
      "metadata": {
        "id": "X9-yb648v8bV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_feat.drop('lang',axis=1)\n",
        "y = encode(train_feat['lang'])\n",
        "\n",
        "#Define model\n",
        "model = Sequential()\n",
        "model.add(Dense(500, input_dim=670, activation='relu'))\n",
        "model.add(Dense(500, activation='relu'))\n",
        "model.add(Dense(250, activation='relu'))\n",
        "model.add(Dense(6, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "#Train model\n",
        "model.fit(x, y, epochs=4, batch_size=None)"
      ],
      "metadata": {
        "id": "AKXkzwgUwQvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a SpeechRecognition recognizer\n",
        "!pip install PyAudio‑0.2.11‑cp37‑cp37m‑win_amd64.whl\n",
        "\n",
        "r = sr.Recognizer()\n",
        "\n",
        "# Record audio from the microphone\n",
        "with sr.Microphone() as source:\n",
        "    print(\"Speak now...\")\n",
        "    audio = r.listen(source)\n",
        "\n",
        "# Use Google's speech recognition to get the spoken text\n",
        "text = r.recognize_google(audio, language=langs)\n",
        "\n",
        "# Preprocess the spoken input\n",
        "text = [text]\n",
        "\n",
        "# Vectorize the spoken input using the same vectorizer used for training\n",
        "input_matrix = vectorizer.transform(text)\n",
        "\n",
        "# Scale the input matrix using the minimum and maximum values from the training data\n",
        "input_matrix = (input_matrix - train_min) / (train_max - train_min)\n",
        "\n",
        "# Predict the language of the spoken input\n",
        "predicted_lang = encoder.inverse_transform(np.argmax(model.predict(input_matrix), axis=-1))[0]\n",
        "\n",
        "# Print the predicted language\n",
        "print(\"Spoken language: \", predicted_lang)"
      ],
      "metadata": {
        "id": "WOOsJXTqFctl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize CountVectorizer with the vocabulary\n",
        "vectorizer = CountVectorizer(analyzer='char', ngram_range=(3, 3), vocabulary=vocab)\n",
        "\n",
        "# Fit and transform the training corpus using the vectorizer\n",
        "input_text = input(\"Enter a sentence: \")\n",
        "train_feat_matrix = vectorizer.transform([input_text])\n",
        "input_feat = pd.DataFrame(data=train_feat_matrix.toarray(),columns=feature_names)\n",
        "input_feat = (input_feat - train_min)/(train_max-train_min)\n",
        "\n",
        "input_pred = model.predict(input_feat)\n",
        "input_lang = encoder.inverse_transform(np.argmax(input_pred, axis=-1))\n",
        "\n",
        "print(\"The language of the input sentence is:\", input_lang[0])"
      ],
      "metadata": {
        "id": "oTAzLWLei-CP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = test_feat.drop('lang', axis=1)\n",
        "y_test = test_feat['lang']\n",
        "\n",
        "# Load the model from the saved file\n",
        "#model = tf.keras.models.load_model('my_model.h5')\n",
        "\n",
        "#Get predictions on test set\n",
        "labels = np.argmax(model.predict(x_test), axis=-1)\n",
        "predictions = encoder.inverse_transform(labels)\n",
        "\n",
        "#Accuracy on test set\n",
        "accuracy = accuracy_score(y_test,predictions)\n",
        "print(accuracy)\n",
        "\n",
        "#Create confusion matrix\n",
        "lang = ['eng','fra','spa', 'hin', 'ara', 'jpn']\n",
        "conf_matrix = confusion_matrix(y_test,predictions)\n",
        "conf_matrix_df = pd.DataFrame(conf_matrix,columns=lang,index=lang)\n",
        "\n",
        "#Plot confusion matrix heatmap\n",
        "plt.figure(figsize=(10, 10), facecolor='w', edgecolor='k')\n",
        "sns.set(font_scale=1.5)\n",
        "sns.heatmap(conf_matrix_df,cmap='coolwarm',annot=True,fmt='.5g',cbar=False)\n",
        "plt.xlabel('Predicted',fontsize=22)\n",
        "plt.ylabel('Actual',fontsize=22)\n"
      ],
      "metadata": {
        "id": "-QsRXRt0v-RA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}